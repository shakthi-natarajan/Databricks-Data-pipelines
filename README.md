Using Databricks Delta tables, created data pipelines that start with ingesting raw data into bronze tables, performing cleaning and transformation to convert it into a silver table and perform aggregations and loading it into gold level tables. As part of transformation, modified and repaired records, created new columns, and merged late-arriving data. Used window function to interpolate missing values, updated Delta tables and checked the version history in Delta table. Also optimized queries using Delta Engine.
Using Pyspark ingested 3M+ records from csv file into Spark dataframe with a defined schema, performed transformation that includes handling null values, creating new calculated columns, filtered data and performed aggregation forr gaining insights on data.
